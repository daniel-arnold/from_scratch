{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "democratic-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specialized-benjamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sepal Length  Sepal Width  Petal Length  Petal Width      Target\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333    1.000000\n",
       "std        0.828066     0.435866      1.765298     0.762238    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#convert to dataframe\n",
    "df = pd.DataFrame(iris.data, columns = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width'])\n",
    "df.insert(4,'Target',iris.target)\n",
    "# target = [0,1,2] corresponds to Setosa, Versicolour, and Virginica, resp.\n",
    "\n",
    "#check head of df\n",
    "df.head()\n",
    "#description of df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rough-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features and labels\n",
    "\n",
    "#feature data\n",
    "X =  df[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']].values\n",
    "\n",
    "#remove the mean\n",
    "X_mean = np.mean(X,axis=0)\n",
    "X_no_mean = X - X_mean\n",
    "\n",
    "#add columm of ones to compute intercept value\n",
    "X_aug = np.concatenate( (X_no_mean,np.ones((len(X),1))) , axis=1)\n",
    "\n",
    "#targets\n",
    "y = df[['Target']].values\n",
    "\n",
    "#one hot encoding of y_train\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0],[1],[2]])\n",
    "y_OH = enc.transform(y).toarray()\n",
    "\n",
    "#split data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_aug, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decreased-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax function (logits)\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z,axis=1).reshape(-1,1)\n",
    "\n",
    "#objective function\n",
    "def logprobs(probs, y_one_hot):\n",
    "    return -np.mean(np.sum(y_one_hot * np.log(probs),axis=1))\n",
    "\n",
    "#compute probabilities (normalized scores)\n",
    "def compute_probs(X, theta):\n",
    "    return softmax(np.dot(X,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "portuguese-destination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature weights: [[-0.42348033  0.96739605 -2.51712086 -1.07936828]\n",
      " [ 0.53444333 -0.32162603 -0.20642703 -0.94425181]\n",
      " [-0.110963   -0.64577002  2.7235479   2.02362009]]\n",
      "intercepts: [-0.42151116  2.46887351 -2.04736235]\n",
      "sklearn loss:  0.11963709827020219\n",
      "skelarn accuracy:  0.9822222222222222\n"
     ]
    }
   ],
   "source": [
    "#solve with sklearn\n",
    "#sklearn logistic regression\n",
    "c = 1 #regularization coefficient (expressed as 1/c in sklearn)\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs',fit_intercept=True,C=c)\n",
    "softmax_reg.fit(X_no_mean,y.squeeze())\n",
    "softmax_reg.get_params()\n",
    "coefs = softmax_reg.coef_\n",
    "intercept = softmax_reg.intercept_\n",
    "print(\"feature weights:\", coefs)\n",
    "print(\"intercepts:\", intercept)\n",
    "\n",
    "theta_sklearn = np.concatenate((coefs, intercept.reshape(-1,1)), axis=1)\n",
    "probs = compute_probs(X_aug, theta_sklearn.T)\n",
    "loss_sklearn = logprobs(probs, y_OH)\n",
    "print('sklearn loss: ', loss_sklearn)\n",
    "\n",
    "sklearn_scores = np.round(probs)\n",
    "sklearn_accuracy = (y_OH == sklearn_scores).sum().astype(float) / len(y_OH) / 3\n",
    "print('skelarn accuracy: ', sklearn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "running-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 - loss: 1.0986122886681096\n",
      "iteration:  2000 - loss: 0.12653808506368472\n",
      "iteration:  4000 - loss: 0.12051464992086894\n",
      "iteration:  6000 - loss: 0.11980823400344635\n",
      "iteration:  8000 - loss: 0.11968142996558431\n",
      "iteration:  10000 - loss: 0.11965010104112833\n",
      "iteration:  12000 - loss: 0.11964091896023404\n",
      "iteration:  14000 - loss: 0.11963804155197351\n",
      "iteration:  16000 - loss: 0.11963711898702958\n",
      "iteration:  18000 - loss: 0.11963682091784501\n",
      "final log liklihood loss:  0.11963672437977206\n",
      "scratch coefficients:  [[-0.42351163  0.53446451 -0.11094804]\n",
      " [ 0.9673487  -0.32158537 -0.64575849]\n",
      " [-2.5171512  -0.20638836  2.7235444 ]\n",
      " [-1.07933591 -0.94429604  2.02363679]\n",
      " [ 0.57841986  3.46889278 -1.04731264]]\n"
     ]
    }
   ],
   "source": [
    "#Solve from scratch\n",
    "\n",
    "max_iters = 20000\n",
    "alpha = 1e-1\n",
    "theta0 = np.ones([X_aug.shape[1], y_OH.shape[1]])\n",
    "theta = []\n",
    "theta.append(theta0)\n",
    "k = 0\n",
    "\n",
    "ll = []\n",
    "\n",
    "while k < max_iters:\n",
    "    \n",
    "    #compute probability scores (normalize logits through softmax)\n",
    "    probs = compute_probs(X_aug,theta[k])\n",
    "    #compute prediction error\n",
    "    e = y_OH - probs\n",
    "    \n",
    "    #compute gradient\n",
    "    #don't penalize the bias term\n",
    "    theta_regularization = np.copy(theta[k])\n",
    "    theta_regularization[4,::]=0\n",
    "    grad = 1/len(X_aug) * (np.dot(X_aug.T, e) - 1/c * theta_regularization)\n",
    "    \n",
    "    #update weights with gradient ascent\n",
    "    theta.append(theta[k] + alpha * grad)\n",
    "    \n",
    "    #compute loss function\n",
    "    loss = logprobs(probs, y_OH)\n",
    "    ll.append(loss)\n",
    "    if k%2000 == 0:\n",
    "        print('iteration: ', k, '- loss:', loss)\n",
    "    k+=1\n",
    "    \n",
    "print('final log liklihood loss: ', loss)\n",
    "print('scratch coefficients: ', theta[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "painted-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch accuracy:  0.9822222222222222\n",
      "sklearn accuracy:  0.9822222222222222\n"
     ]
    }
   ],
   "source": [
    "# compare scores\n",
    "probs = compute_probs(X_aug,theta[k])\n",
    "scratch_accuracy = (y_OH == np.round(probs)).sum().astype(float) / len(y_OH) / 3\n",
    "print('scratch accuracy: ', scratch_accuracy)\n",
    "print('sklearn accuracy: ', sklearn_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
